{"cells":[{"source":"# 0. 降维概述\n许多机器学习问题涉及成百上千的特征，应用庞大的特征量往往会造成许多问题：\n1. 训练速度大大降低，很难寻找好的解决方案\n2. 过高的数据维度更可能会产生过拟合，导致模型泛化能力弱。\n3. 高维数据具有稀疏性，许多训练实例之间可能距离很远，找到数据规律更困难\n解决以上问题，可以使用降维的方法。降维通过过滤掉一些不必要的冗余信息和细节信息，能够使模型更加准确，有效地加快训练速度。\n\n# 1. 移除低方差特征\n## 原理简析：\n移除低方差是特征选择的一个基本方法。低方差说明该特征在每组数据间变化不大（极端情况下一直不变）。这样的特征对数据影响较小，可以优先剔除。\n## 代码示例：","cell_type":"markdown","metadata":{"_id":"AAD694E337714FEE87B3949D5801C651","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"227F36CBB71045378A3DBF507FB1813A","trusted":true},"originalIndex":0,"unchanged":false,"replace":true},{"metadata":{"_id":"0EE3BCD0D8B442E5B89E189B4B694CB6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"205C3C4B00164BA5A92A6B4E17D1818A","trusted":true},"cell_type":"code","outputs":[],"source":"from sklearn.feature_selection import VarianceThreshold\n# 示例：布尔值数据集，每个特征都是两点分布\nX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n# 剔除某一个值概率超过0.8的特征 方差最小为0.8*0.2=0.16\nsel = VarianceThreshold(threshold=0.16)\nsel.fit_transform(X)","execution_count":1,"unchanged":true},{"metadata":{"_id":"1B8E381EEC514C11B6DD18D00F43DB37","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A64CE1AC3C70479D98EBEA3874520D03","trusted":true},"cell_type":"markdown","source":"# 2. 单变量特征选择\n单变量的特征选择是通过基于单变量的统计测试来选择最好的特征。简单的处理方法有：\n1. 移除除了评分最高K个以外的特征`SelectKBest()`\n2. p移除除了排名在一定百分比以外的特征`SelectPercentile()`\n3. 对于单变量的检验方法，常用的有F检验，互信息方法（需要较多数据），卡方检验（只适用于分类问题）\n   * 对于回归: f_regression , mutual_info_regression\n   * 对于分类: chi2 , f_classif , mutual_info_classif","unchanged":true},{"metadata":{"_id":"BFE412C6E1CB4B5C8CB6C358C64919AC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D0FCD559ED9B4CDB8C4CB65F375F0404","trusted":true},"cell_type":"code","outputs":[],"source":"# 导入鸢尾花数据集\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX, y = iris.data, iris.target\nprint(X.shape)\n# 导入SelectKBest方法和检测指标f_classif（F检验）\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nX_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\nprint(X_new.shape)","execution_count":1,"unchanged":true},{"metadata":{"_id":"C85534DCB2C94EE68AEB2081E948590E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9221B8636C2A414F801E2B1FFB0CC9B1","trusted":true},"cell_type":"markdown","source":"# 3. 递归式特征消除\n递归式特征消除法使用某种估计器每次移除最不重要的特征，再在移除后的小集合中重复这个过程，通过交叉循环找到最佳的特征数量选择。\n## 代码示例：递归式特征消除法进行重要性排名","unchanged":true},{"metadata":{"_id":"C2FE49022B094856ACB5DBBD864D3952","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FC58A95EFB04F589DED7F3CC07DEAAC","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt\n# 加载数据集\ndigits = load_digits()\nX = digits.data\ny = digits.target\nprint(X.shape)\n# 创建RFE对象并进行排名\nsvc = SVC(kernel=\"linear\", C=1)\nrfe = RFE(estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n# 根据排名ranking进行作图\nplt.matshow(ranking)\nplt.colorbar()\nplt.title(\"Ranking of pixels with RFE\")\nplt.show()","execution_count":2,"outputs":[],"unchanged":true},{"metadata":{"_id":"E623FD471B814D43BC34F4B3558D3FEC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"55CE4B1E75A346049A21DC90E4A51234","trusted":true},"cell_type":"markdown","source":"\n## 代码示例：递归式特征消除法自动调整特征数","unchanged":true},{"metadata":{"_id":"A8501E295A2241AAB1D14307493A7682","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C9C165F3648D407A89127BA8C79F3EA1","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_classification\n# 使用三个特征进行交叉验证\nX, y = make_classification(n_samples=1000, n_features=25, n_informative=3,\n                           n_redundant=2, n_repeated=0, n_classes=8,\n                           n_clusters_per_class=1, random_state=0)\n# 创建RFE对象，计算交叉验证分数（准确度得分）\nsvc = SVC(kernel=\"linear\")\n# 使用rfecv函数，迭代到最佳特征数\nmin_features_to_select = 1 \nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n              scoring='accuracy',\n              min_features_to_select=min_features_to_select)\nrfecv.fit(X, y)\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)","execution_count":3,"outputs":[],"unchanged":true},{"metadata":{"_id":"4B55AD74BD654A7C8E2BA67B578E5782","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"753DFA5624CC4BAFBF93BC0F922F001F","trusted":true},"cell_type":"code","outputs":[],"source":"# 作图：每个特征的交叉验证分数\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(min_features_to_select,\n               len(rfecv.grid_scores_) + min_features_to_select),\n         rfecv.grid_scores_)\nplt.show()","execution_count":4,"unchanged":true},{"metadata":{"_id":"177A5F665E844AE98166E17C4A630407","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0A0589CA21FF4ABA9D25E3073B5E5EF2","trusted":true},"cell_type":"markdown","source":"## 4. L1正则化方法\nL1正则化方法能够将数据稀疏化，在数据更加分散的过程中，许多系数逐渐归0。该方法可以和SelectModel方法一起使用，选取在稀疏过程中没有为0系数的特征。","unchanged":true},{"metadata":{"_id":"1D0F2F1F87C5450CA352F92A9DB152C6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F7F28BBBF629492B86AC6370DFCA57F7","trusted":true},"cell_type":"code","source":"# 导入鸢尾花数据集\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX, y = iris.data, iris.target\nprint(X.shape)\n# 使用LinearSVC方法\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X)\nprint(X_new.shape)","execution_count":5,"outputs":[],"unchanged":true},{"metadata":{"_id":"148E8727A4E943649CC78C2ECB5001E8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DB1D8D3A689F46E684ECF67AE6BA291B","trusted":true},"cell_type":"markdown","source":"## 5. 基于树的特征选取\n使用树中的estimators模块，也能够消除一些不必要的特征","unchanged":true},{"metadata":{"_id":"FBA8BAD9C28D4CC08576FC1D6B2A5C55","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8D86A541B91C4834AFE2B0853A3641AA","trusted":true},"cell_type":"code","outputs":[],"source":"# 导入鸢尾花数据集\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\niris = load_iris()\nX, y = iris.data, iris.target\nX.shape","execution_count":6,"unchanged":true},{"metadata":{"_id":"8E592ABC5D4444A1AC09FD9C863C14AE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"94771CD2D9654163814D50C8D322DA48","trusted":true},"cell_type":"code","outputs":[],"source":"clf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(X, y)\nclf.feature_importances_  ","execution_count":7,"unchanged":true},{"metadata":{"_id":"076D181AF82C44678E5837010B993F7E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6808DE249B834FE2941C8A5BB17F17DF","trusted":true},"cell_type":"code","outputs":[],"source":"model = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nX_new.shape                         ","execution_count":8,"unchanged":true},{"metadata":{"_id":"CD355658D4F944B28C4CFAB1B386097A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"02CFA8014DAD46CE82E89220C27611D8","trusted":true},"cell_type":"markdown","source":"# 参考资料：\nsk-learn官方文档\n\n# 作业\n将卡方分布(chi2)为标准的单变量特征选择方法应用于乳腺癌数据集，输出包含前20个评分最高特征的数据集。","unchanged":true},{"metadata":{"_id":"A3AF7260AC4B4A3A8094F057FE1CC9B3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C4EA878FD192448B948F42B8FF879534","trusted":true},"cell_type":"code","outputs":[],"source":"# 导入乳腺癌数据集\ndata = datasets.load_breast_cancer()\nX = data.data\ny = data.target","execution_count":9,"unchanged":true},{"metadata":{"_id":"CCF7562A5C114216919BB140F5A3F928","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7E6F67A8B79C44C8BC09ED31534C8ED6","trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null,"originalIndex":17,"unchanged":false,"removerange":true}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}